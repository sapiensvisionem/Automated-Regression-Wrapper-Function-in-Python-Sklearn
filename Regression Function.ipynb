{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(data, location): # location needs to be of the format 'C:/Users/jihun/Documents/''\n",
    "    # This function uses the following models:\n",
    "    \n",
    "        # linear regression\n",
    "        # 1. OLS\n",
    "        # 2. lasso\n",
    "        # 3. ridge\n",
    "        # 4. elastic net\n",
    "        # 5. least angle regression\n",
    "        # 6. bayesian regression\n",
    "        # 7. robust regression\n",
    "        \n",
    "        # kernel\n",
    "        # 9. kernel ridge regression\n",
    "        \n",
    "        # nearest neighbors\n",
    "        # 8. knn\n",
    "        \n",
    "        # support veector machine\n",
    "        # 7. linear\n",
    "        # 8. polynomial\n",
    "        # 9. radial\n",
    "        \n",
    "        # gaussian process\n",
    "        # 10 gaussian process regression\n",
    "        \n",
    "        # tree methods\n",
    "        # 11. random forest\n",
    "        # 12. adaboost\n",
    "        # 13. gradient boosting\n",
    "        \n",
    "        # neural network\n",
    "        # 15. multilayer perceptron\n",
    " \n",
    "    # import necessary libraries\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVR\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel,  RBF\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "    from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, Lars, BayesianRidge, HuberRegressor\n",
    "\n",
    "    # define response and feature\n",
    "    y = data.iloc[:,0] # response variable needs to be the first column\n",
    "    X = data.iloc[:,1:]\n",
    "    \n",
    "    # split train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # model initializers\n",
    "    models = [\n",
    "        LinearRegression(),\n",
    "        Ridge(),\n",
    "        Lasso(),\n",
    "        ElasticNet(),\n",
    "        Lars(),\n",
    "        BayesianRidge(),\n",
    "        HuberRegressor(),\n",
    "        KernelRidge(),\n",
    "        KNeighborsRegressor(),\n",
    "        GaussianProcessRegressor(),\n",
    "        RandomForestRegressor(), \n",
    "        AdaBoostRegressor(), \n",
    "        GradientBoostingRegressor(),\n",
    "        MLPRegressor()]\n",
    "    \n",
    "    # fit a standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # parameter space for grid search\n",
    "    parameter_grid=[\n",
    "        # Linear Regression\n",
    "        {'fit_intercept':[True]},\n",
    "        # Lasso Regression\n",
    "        {'alpha':np.linspace(0.1,1,10)}, # Constant that multiplies the L1 term. \n",
    "        # Ridge Regression\n",
    "        {'alpha':np.linspace(0.1,1,10)}, # Regularization strength; Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization\n",
    "        # Elastic Net\n",
    "        {'alpha':np.linspace(0.1,1,10),  \n",
    "        'l1_ratio': np.linspace(0.1,0.9,9)},\n",
    "        # Least angle regression\n",
    "        {'fit_intercept':[True]}, # Regularizes the covariance estimate as (1-reg_param)*Sigma + reg_param*np.eye(n_features)\n",
    "        # Bayesian Ridge\n",
    "        {'alpha_1':[1e-6,1], # Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter\n",
    "         'alpha_2':[1e-6,1], # Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter\n",
    "         'lambda_1':[1e-6,1], # Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter\n",
    "         'lambda_2':[1e-6,1]}, # Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter\n",
    "        # Huber Regression\n",
    "        {'alpha':[0.001,0.01,0.05,0.1,0.5,1]}, # Regularization parameter.\n",
    "        # Kernel Ridge Regression\n",
    "        {'gamma':['RBF', 'laplacian', 'polynomial', 'exponential chi2'], # Kernel mapping used internally\n",
    "        'alpha':[0.001,0.01,0.05,0.1,0.5,1]}, # Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates\n",
    "        # K Nearest Neighbor\n",
    "        {'n_neighbors': [3,5,7,9,11], # Number of neighbors to use\n",
    "        'weights':['uniform','distance']}, # weight function used in prediction\n",
    "        # Gaussian Process\n",
    "        {'optimizer':['fmin_l_bbfgs_b']},\n",
    "        # Support Vector Classifier with Linear Kernel\n",
    "        {'C':[0.001,0.01,0.05,0.1,0.2,0.5,1]}, # regularization parameter; The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "        # Support Vector Classifier with Polynomial Kernel\n",
    "        {'C':[0.001,0.01,0.05,0.1,0.2,0.5,1],\n",
    "        'degree':[1,2,3], # Degree of the polynomial kernel function\n",
    "        'gamma':['scale','auto']}, # Kernel coefficient\n",
    "        # Support Vector Classifier with RBF Kernel\n",
    "        {'C':[0.001,0.01,0.05,0.1,0.2,0.5,1],\n",
    "        'gamma':['scale','auto']}, \n",
    "        # Random Forest\n",
    "        {'n_estimators': [500, 1000, 1500],\n",
    "         'max_features': ['auto','log2'],\n",
    "         'criterion' :['gini', 'entropy']},\n",
    "        # Adaboost\n",
    "        {'n_estimators': [500, 1000, 1500],\n",
    "         'learning_rate':[0.01,0.05,0.1,0.5,1,2]}, # Learning rate shrinks the contribution of each regressor \n",
    "        # Gradient Boosting\n",
    "        {'n_estimators': [500, 1000, 1500],\n",
    "         'learning_rate':[0.01,0.05,0.1,0.5,1,2], # learning rate shrinks the contribution of each tree\n",
    "         'loss':['deviance','exponential'],\n",
    "         'subsample':[0.7,0.85,1.0], # The fraction of samples to be used for fitting the individual base learners\n",
    "         'max_depth': [2,3,4,5], # maximum depth of the individual regression estimators.\n",
    "         'max_features':['auto']},\n",
    "        # Multilayer Perceptron\n",
    "        {'solver': ['lbfgs'],\n",
    "        'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"], # Learning rate schedule for weight updates\n",
    "        'hidden_layer_sizes':[(5,5,5), (7,5,3), (10,)], # The ith element represents the number of neurons in the ith hidden layer\n",
    "        'alpha': [0.001,0.01,0.1,1], # L2 penalty (regularization term) parameter\n",
    "        'activation': [\"relu\", \"tanh\", 'identity'], # Activation function for the hidden layer\n",
    "        'random_state':[97]}\n",
    "    ] \n",
    "\n",
    "    modList = []\n",
    "    result = np.repeat(1,3)\n",
    "    \n",
    "    colnames = ['r-squared',\n",
    "               'mean squared error',\n",
    "               'mean absolute error']\n",
    "        \n",
    "    rownames = [1,\n",
    "                'linear regression',\n",
    "                'lasso regression',\n",
    "                'ridge regression',\n",
    "                'elastic net',\n",
    "                'least angle regression',\n",
    "                'bayesian regression',\n",
    "                'robust regression',\n",
    "                'kernel ridge regression',\n",
    "                'knn',\n",
    "                'svm-linear',\n",
    "                'svm-polynomial',\n",
    "                'svm-rbf',\n",
    "                'gaussian process',\n",
    "                'random forest',\n",
    "                'adaboost',\n",
    "                'gradient boosting',\n",
    "                'multilayer perceptron']\n",
    "    \n",
    "    # fit models via loop\n",
    "    for clf,grid,name in zip(models, parameter_grid, rownames):\n",
    "        # gridsearch\n",
    "        mod = GridSearchCV(estimator=clf,\n",
    "              param_grid=grid,\n",
    "              cv=10)\n",
    "        mod.fit(X_train,y_train)\n",
    "        # save the model\n",
    "        joblib.dump(clf, location+name+'.pkl') \n",
    "        modList.append(mod)\n",
    "        # make prediction on test set\n",
    "        predictions = mod.predict(X_test)  \n",
    "        # calculate classification metrics\n",
    "        metrics = np.array([\n",
    "            accuracy_score(y_test, predictions),\n",
    "            roc_auc_score(y_test, predictions),\n",
    "            balanced_accuracy_score(y_test, predictions),\n",
    "            cohen_kappa_score(y_test, predictions),\n",
    "            f1_score(y_test, predictions),\n",
    "            recall_score(y_test, predictions),\n",
    "            precision_score(y_test, predictions),\n",
    "            average_precision_score(y_test, predictions),\n",
    "            brier_score_loss(y_test, predictions)\n",
    "        ])\n",
    "        # add the metrics to the result\n",
    "        result = np.vstack((result,metrics))\n",
    "        \n",
    "    \n",
    "    # save the final classification metric table\n",
    "\n",
    "    result = pd.DataFrame(result, columns=colnames, index=rownames)\n",
    "    result.to_csv(location + 'classification_model_results.csv')\n",
    "    \n",
    "    return result\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
